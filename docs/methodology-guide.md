# Methodology Guide

## Advanced Validation Techniques

This guide goes deeper into validation methodologies, experiments, and best practices for product leaders who want to master business idea validation.

## Validation Frameworks

### 1. Lean Canvas (Primary)

**Origin**: Ash Maurya (adaptation of Business Model Canvas)

**Purpose**: One-page business model for startups

**When to use**:
- New startup or product line
- Pre-product validation
- Pitch preparation
- Team alignment

**Core principle**: Document your plan, identify riskiest assumptions, test systematically

**9 Building Blocks**:
1. Problem (top 3)
2. Customer Segments
3. Unique Value Proposition
4. Solution (top 3 features)
5. Channels
6. Revenue Streams
7. Cost Structure
8. Key Metrics
9. Unfair Advantage

**Validation sequence**:
1. Problem ← Start here (talk to customers)
2. Customer Segments ← Verify who has the problem
3. Value Proposition ← Test messaging
4. Solution ← Prototype/mockup
5. Revenue & Costs ← Test pricing
6. Channels ← Validate acquisition
7. Scale ← Optimize and grow

### 2. SVPG Opportunity Assessment

**Origin**: Marty Cagan / Silicon Valley Product Group

**Purpose**: Evaluate product opportunities for existing products

**When to use**:
- Existing product with active users
- Feature prioritization
- Roadmap planning
- Resource allocation decisions

**Core principle**: Discover what to build through continuous customer engagement

**10 Questions**:
1. What problem will this solve?
2. For whom?
3. How big is the opportunity?
4. What alternatives exist?
5. Why are we best suited?
6. Why now?
7. How will we get to market?
8. How will we measure success?
9. What's critical to success?
10. What are the risks?

**Four Risks Framework**:
1. **Value risk**: Will customers buy/use it?
2. **Usability risk**: Can they figure out how to use it?
3. **Feasibility risk**: Can we build it?
4. **Business viability risk**: Does it work for our business?

**Test order**: Value → Usability → Feasibility → Viability

### 3. Value Proposition Canvas

**Origin**: Alexander Osterwalder

**Purpose**: Achieve product-market fit by understanding customer jobs, pains, and gains

**When to use**:
- Deep customer discovery
- Positioning/messaging
- Feature prioritization
- Jobs-to-be-done research

**Two sides**:

**Customer Profile**:
- Customer jobs (what they're trying to do)
- Pains (obstacles, risks, bad outcomes)
- Gains (benefits they want, success criteria)

**Value Map**:
- Products & Services (what you offer)
- Pain Relievers (how you alleviate pains)
- Gain Creators (how you create gains)

**Fit**: When your pain relievers and gain creators match their pains and gains

### 4. Jobs-to-be-Done (JTBD)

**Origin**: Clayton Christensen

**Purpose**: Understand why customers "hire" products

**Core insight**: People don't want a 1/4 inch drill, they want a 1/4 inch hole

**Framework**:
- **Functional job**: What task are they trying to accomplish?
- **Emotional job**: How do they want to feel?
- **Social job**: How do they want to be perceived?

**Example**: People don't hire Uber for transportation (functional), they hire it for reliability and status (emotional/social)

**Use JTBD when**:
- Customers can't articulate the problem
- You need to understand switching behavior
- You're defining category positioning

### 5. Pretotyping

**Origin**: Alberto Savoia (Google)

**Purpose**: Test if "we're building the right it before we build it right"

**Principle**: Create the simplest fake version to test demand

**Examples**:
- **Mechanical Turk**: Fake AI with humans behind it
- **Smoke test**: Landing page with "Sign up" before product exists
- **Concierge**: Deliver service manually before automating
- **Wizard of Oz**: Fake the product, manually operate behind scenes

**When to use**: Before writing any code

## Validation Experiments

### Problem Validation

**Goal**: Confirm the problem is real and worth solving

#### Customer Interviews
**Method**: Talk to 10-15 potential customers

**Questions**:
- "Tell me about the last time you [experienced problem]"
- "How do you solve this today?"
- "What's most frustrating about current solution?"
- "How much time/money does this cost you?"
- "If we solved this, what would change?"

**Good answers**:
- Specific stories with details
- Quantified impact (hours, dollars)
- Strong emotion (frustration, pain)
- Current workarounds exist

**Red flags**:
- Can't think of specific examples
- "It would be nice but not critical"
- No current solution (might mean no real problem)
- No willingness to change behavior

#### Problem Ranking
**Method**: Show top 3 problems, ask customers to rank

**Validation**: If your #1 problem matches their #1 problem → good fit

#### Observe Current Behavior
**Method**: Watch them use current solution

**Look for**:
- Where do they struggle?
- What do they complain about?
- What workarounds do they create?

### Solution Validation

**Goal**: Confirm your solution solves the problem

#### Mockups/Wireframes
**Method**: Show designs (no code)

**Test**:
- Do they understand it?
- Does it solve their problem?
- What would they change?

**Tool**: Figma, Balsamiq, sketches

#### Clickable Prototype
**Method**: Interactive mockup

**Test**:
- Can they complete key tasks?
- Where do they get confused?
- Do they see the value?

**Tools**: Figma, InVision, Marvel

#### Concierge MVP
**Method**: Manually deliver the service

**Example**: Food delivery app → You manually take orders and deliver food

**Validates**: Problem + willingness to pay, before building tech

#### Wizard of Oz
**Method**: Fake the product, operate manually behind scenes

**Example**: AI chatbot → Humans answer questions, pretend it's AI

**Validates**: User experience + value, before building AI

### Pricing Validation

**Goal**: Confirm willingness to pay and price point

#### Direct Question (Unreliable)
❌ "Would you pay $X for this?"
Everyone says yes, few actually pay

#### Better Approaches

**Van Westendorp Method**:
- At what price is this too cheap (suspicious)?
- At what price is this a bargain?
- At what price is this expensive but worth it?
- At what price is this too expensive?

**Optimal price**: Where "expensive but worth it" and "bargain" cross

**Comparable Pricing**:
- What do they pay for current solution?
- What's their budget for this category?
- What % of current spend is your solution?

**Pre-orders/Deposits**:
- "Reserve your spot for $99"
- Money = real validation
- Talk is cheap, payment isn't

**A/B Testing**:
- Show different prices to different segments
- Measure conversion rate
- Find optimal price point

### Channel Validation

**Goal**: Confirm you can reach customers profitably

#### Content Marketing
**Test**: Write 3-5 blog posts, measure traffic and conversions

**Validates**: Inbound interest, SEO viability

#### Paid Ads
**Test**: $500-1000 on Google/Facebook ads

**Measure**:
- Cost per click (CPC)
- Conversion rate
- Customer acquisition cost (CAC)

**Validates**: Can you acquire customers for < LTV?

#### Partnerships
**Test**: Approach 3-5 potential partners

**Validates**: Access to distribution, co-selling opportunity

#### Direct Outreach
**Test**: Cold email/LinkedIn 50-100 prospects

**Measure**: Response rate, meeting rate, interest level

**Validates**: Message resonance, demand

### Market Size Validation

**Goal**: Confirm market is large enough

#### Top-Down (Less reliable)
- Total market size from analyst reports
- Your addressable portion
- Your realistic capture rate

**Example**: "$100B market, we can get 1%" ← Usually wrong

#### Bottom-Up (More reliable)
- Number of target customers
- Price per customer
- Total = number × price

**Example**: "10,000 companies × $10K/year = $100M SAM"

#### TAM/SAM/SOM
- **TAM** (Total Addressable Market): If you had 100% of everyone who could use this
- **SAM** (Serviceable Addressable Market): Realistic segment you can serve
- **SOM** (Serviceable Obtainable Market): What you can actually capture in 3-5 years

**Minimum viable market**: Can you build a $10M+ business? If not, might be too small.

## Validation Best Practices

### Talk to Customers Early and Often

**Minimum**: 10-15 interviews before building
**Ideal**: Continuous customer conversations

**How often**:
- Pre-build: 10-15 interviews (problem validation)
- During build: 5-10 users/week (usability testing)
- Post-launch: Ongoing (feedback, retention)

### Test Riskiest Assumption First

**Order risks**:
1. **Highest impact** + **Most uncertain** = Test first
2. Medium impact or uncertainty = Test second
3. Low impact or highly certain = Test last (or don't test)

**Example**:
- High risk: "Customers will pay $99/month" → Test first
- Medium risk: "We can build in 3 months" → Test second
- Low risk: "Email works better than SMS" → Test last

### Use Evidence, Not Opinions

**Weak validation**:
- "My friend said it's a good idea"
- "I think customers would pay for this"
- "Everyone I know has this problem"

**Strong validation**:
- "15 customers interviewed, 12 said they'd pay $99"
- "100 people signed up on landing page in 2 weeks"
- "5 customers pre-paid for beta access"

### Embrace Invalidation

**Mindset shift**: Finding out you're wrong early is a win

**Cost of learning**:
- Customer interview: 1 hour
- Prototype: 1-2 days
- MVP: 1-3 months
- Full product: 6-12 months

**If you're wrong**:
- After interview: Lost 1 hour
- After prototype: Lost 1-2 days
- After MVP: Lost 1-3 months
- After full product: Lost 6-12 months (and money)

**Better to pivot after 1 hour than after 6 months**

### Iterate Based on Learning

Canvas is living document:
- Interview 10 customers → Update problem/segment
- Test pricing → Update revenue model
- Build prototype → Update solution
- Run experiment → Update assumptions

**Update weekly as you learn**

## Advanced Topics

### Assumption Mapping

**Method**: Map all assumptions on 2x2 matrix

**Axes**:
- X: How important is this assumption?
- Y: How much evidence do we have?

**Quadrants**:
1. **Critical + Unvalidated**: Test immediately
2. **Critical + Validated**: Monitor
3. **Less important + Unvalidated**: Test when capacity allows
4. **Less important + Validated**: Don't worry about

**Test everything in Quadrant 1 before building**

### Pirate Metrics (AARRR)

**Framework** for tracking product success:

1. **Acquisition**: How do users find you?
2. **Activation**: Do they have a great first experience?
3. **Retention**: Do they come back?
4. **Revenue**: Can you monetize?
5. **Referral**: Do they tell others?

**Validation focus**:
- Pre-product: Acquisition (can you reach customers?)
- MVP: Activation + Retention (is it valuable?)
- Growth: Revenue + Referral (can you scale?)

### Cohort Analysis

**Method**: Track groups of users who started at the same time

**Example**:
- Week 1 cohort: 100 users → 50 active after 30 days (50% retention)
- Week 2 cohort: 100 users → 60 active after 30 days (60% retention)

**Validation**: Retention improving = product getting better

### Unit Economics

**Key metrics**:
- **LTV** (Lifetime Value): Revenue per customer over their lifetime
- **CAC** (Customer Acquisition Cost): Cost to acquire one customer
- **Payback period**: How long to recover CAC
- **LTV:CAC ratio**: LTV / CAC

**Healthy business**:
- LTV:CAC > 3:1
- Payback period < 12 months
- Gross margin > 70% (SaaS) or 40% (eCommerce)

**Validate economics before scaling**

### Competitive Analysis

**Framework**:

1. **Direct competitors**: Same solution, same customer
2. **Indirect competitors**: Different solution, same customer
3. **Substitutes**: What do customers do if you don't exist?

**Analyze**:
- Positioning
- Pricing
- Features
- Customer reviews (what do they love/hate?)
- Growth trajectory

**Find your wedge**: What can you do better than all of them?

## Common Mistakes

### 1. Building Without Validation
**Mistake**: "Let's build it and see if they come"
**Fix**: Talk to customers first

### 2. Asking Leading Questions
**Mistake**: "Would you use a fast, easy, cheap project management tool?"
**Fix**: Ask about their current behavior, not your idea

### 3. Talking to Friends/Family
**Mistake**: Friends will lie to be nice
**Fix**: Talk to strangers who match your target customer

### 4. Confusing Interest with Commitment
**Mistake**: "10 people said they'd use it!"
**Fix**: Did they pay, pre-order, or give contact info?

### 5. Ignoring Negative Feedback
**Mistake**: "They just don't get it"
**Fix**: If 5+ people don't get it, you need to change it

### 6. Perfect Canvas Syndrome
**Mistake**: Spending weeks perfecting the canvas
**Fix**: Canvas is for identifying assumptions to test, not perfecting on paper

### 7. Skipping the Hardest Questions
**Mistake**: Avoiding "how will you make money?"
**Fix**: Answer the hard questions early

### 8. Testing Too Many Things at Once
**Mistake**: New feature + new price + new channel = can't tell what worked
**Fix**: Change one variable at a time

## Resources

### Books
- **"Running Lean"** - Ash Maurya (Lean Canvas)
- **"The Mom Test"** - Rob Fitzpatrick (Customer interviews)
- **"Testing Business Ideas"** - Bland & Osterwalder (Experiments)
- **"Inspired"** - Marty Cagan (Product discovery)
- **"The Lean Startup"** - Eric Ries (Build-Measure-Learn)
- **"Sprint"** - Jake Knapp (5-day validation)

### Websites
- leanstack.com - Lean Canvas tools
- svpg.com - SVPG articles
- strategyzer.com - Value Proposition Canvas
- 12testing.com - Experiment library

### Courses
- Y Combinator Startup School
- Reforge Product Strategy
- SVPG Workshops

---

**Remember**: The goal is learning, not being right. Validate assumptions quickly, pivot when needed, and build what customers actually want.
